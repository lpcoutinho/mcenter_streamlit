{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "from psycopg2 import sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "ACCESS_TOKEN_BUENOSHOPS = os.getenv(\"ACCESS_TOKEN_BUENOSHOPS\")\n",
    "SELLER_ID_BUENOSHOPS = os.getenv(\"SELLER_ID_BUENOSHOPS\")\n",
    "\n",
    "ACCESS_TOKEN_MUSICALCRIS = os.getenv(\"ACCESS_TOKEN_MUSICALCRIS\")\n",
    "SELLER_ID_MUSICALCRIS = os.getenv(\"SELLER_ID_MUSICALCRIS\")\n",
    "\n",
    "ACCESS_TOKEN_MCENTER = os.getenv(\"ACCESS_TOKEN_MCENTER\")\n",
    "SELLER_ID_MCENTER = os.getenv(\"SELLER_ID_MCENTER\")\n",
    "\n",
    "HOST = os.getenv(\"HOST\")\n",
    "POSTGRES_DB = os.getenv(\"POSTGRES_DB\")\n",
    "POSTGRES_USER = os.getenv(\"POSTGRES_USER\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_config = {\n",
    "    \"host\": HOST,\n",
    "    \"database\": POSTGRES_DB,\n",
    "    \"user\": POSTGRES_USER,\n",
    "    \"password\": POSTGRES_PASSWORD,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_to_db(access_token, seller_id, db_config, table_item):\n",
    "    logger.add(\n",
    "        f\"Data/Output/Log/{table_item}.log\",\n",
    "        rotation=\"10 MB\",\n",
    "        format=\"{time:YYYY-MM-DD at HH:mm:ss} | {level} | {message}\",\n",
    "    )\n",
    "\n",
    "    start_prog = time.time()  # Registra o inicio da aplicação\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    # ACCESS_TOKEN = os.getenv(f\"{access_token}\")\n",
    "    # SELLER_ID = os.getenv(f\"{seller_id}\")\n",
    "    \n",
    "    ACCESS_TOKEN = access_token\n",
    "    SELLER_ID = seller_id\n",
    "    \n",
    "\n",
    "    # Consulta aos itens com logistic_type=fulfillment\n",
    "    base_url = f\"https://api.mercadolibre.com/users/{SELLER_ID}/items/search?logistic_type=fulfillment\"\n",
    "\n",
    "    params = {\n",
    "        \"limit\": 100,\n",
    "        \"offset\": 0,\n",
    "    }\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {ACCESS_TOKEN}\"}\n",
    "\n",
    "    # buscando lista de códigos\n",
    "    json_list = []\n",
    "    try:\n",
    "        while True:\n",
    "            response = requests.get(base_url, headers=headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            logger.info(data)\n",
    "            if \"results\" in data:\n",
    "                json_list.extend(data[\"results\"])\n",
    "                logger.info(data[\"results\"])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            # Verifique se há mais páginas\n",
    "            if \"paging\" in data:\n",
    "                total_data = data[\"paging\"].get(\"total\")\n",
    "\n",
    "                total_pages = math.ceil(total_data / params[\"limit\"])\n",
    "                logger.info(f\"Total de páginas a serem processadas: {total_pages}\")\n",
    "                logger.info(f'Offset atual: {params[\"offset\"]}')\n",
    "\n",
    "                if params[\"offset\"] >= total_pages * params[\"limit\"]:\n",
    "                    break\n",
    "\n",
    "                params[\"offset\"] += params[\"limit\"]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        logger.error(f\"Erro ao fazer a requisição para {base_url}: {req_err}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro não esperado: {e}\")\n",
    "\n",
    "    logger.info(f\"Total esperado de dados: {total_data}\")\n",
    "    logger.info(f\"Total de dados coletados: {len(json_list)}\")\n",
    "\n",
    "    # buscando de itens em json\n",
    "    json_list_item = []\n",
    "    c = 1\n",
    "    for item in json_list:\n",
    "        base_url = f\"https://api.mercadolibre.com/items/{item}\"\n",
    "        headers = {\"Authorization\": f\"Bearer {ACCESS_TOKEN}\"}\n",
    "        t = len(json_list)\n",
    "        logger.info(item)\n",
    "        logger.info(f\"{c}/{t}\")\n",
    "        c += 1\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            json_list_item.append(data)\n",
    "            logger.info(f\"Tamanho da nova lista: {len(json_list_item)}/{t}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Erro ao obter dados para o item {item}: {e}\")\n",
    "\n",
    "        # Se c for um múltiplo de 50, aguarde 1 minuto\n",
    "        if c % 50 == 0:\n",
    "            logger.warning(\"Esperando 1 minuto...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    logger.info(f\"Tamanho da lista de itens: {len(json_list_item)}\")\n",
    "\n",
    "    # Salvando a lista de itens\n",
    "    caminho_arquivo = f\"Data/Output/list_{table_item}.json\"\n",
    "\n",
    "    with open(caminho_arquivo, \"w\") as arquivo:\n",
    "        json.dump(json_list_item, arquivo)\n",
    "\n",
    "    with open(caminho_arquivo, \"r\") as arquivo:\n",
    "        json_list_item = json.load(arquivo)\n",
    "\n",
    "    df = pd.DataFrame(json_list_item)\n",
    "\n",
    "    logger.info(f\"Tamanho do dataframe de itens: {df.shape}\")\n",
    "    df.sample()\n",
    "\n",
    "    # pegando dados em attributes\n",
    "    # attributes: SELLER_SKU\n",
    "    resultados_attributes = []\n",
    "\n",
    "    for item in json_list_item:\n",
    "        # Extrair os valores desejados\n",
    "        first_id = item[\"id\"]\n",
    "        inventory_id = item[\"inventory_id\"]\n",
    "        variations = item[\"variations\"]\n",
    "        status = item[\"status\"]\n",
    "        catalog_product_id = item[\"catalog_product_id\"]\n",
    "        seller_custom_field = item[\"seller_custom_field\"]\n",
    "        catalog_listing = item[\"catalog_listing\"]\n",
    "        logistic_type = item[\"shipping\"][\"logistic_type\"]\n",
    "        item_relations = item[\"item_relations\"]\n",
    "\n",
    "        # Procurar em \"attributes\" onde \"id\" é \"SELLER_SKU\"\n",
    "        seller_sku_entry = next(\n",
    "            (attr for attr in item[\"attributes\"] if attr[\"id\"] == \"SELLER_SKU\"), None\n",
    "        )\n",
    "\n",
    "        # Pegar \"value_name\" e \"value_id\" se a entrada existir, caso contrário, definir como None\n",
    "        attribute_value_name = (\n",
    "            seller_sku_entry[\"value_name\"] if seller_sku_entry else None\n",
    "        )\n",
    "        attribute_value_id = seller_sku_entry[\"value_id\"] if seller_sku_entry else None\n",
    "\n",
    "        # Adicionar os resultados_attributes à lista\n",
    "        resultados_attributes.append(\n",
    "            {\n",
    "                \"ml_code\": first_id,\n",
    "                \"inventory_id\": inventory_id,\n",
    "                \"status\": status,\n",
    "                \"variations\": variations,\n",
    "                \"catalog_listing\": catalog_listing,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_sku = pd.DataFrame(resultados_attributes)\n",
    "\n",
    "\n",
    "    # pegando dados em variations\n",
    "    # variations: variation_id,  attribute_combination: value_id, value_name, seller_sku ,inventory_id\n",
    "    resultados_variations = []\n",
    "\n",
    "    for item in json_list_item:\n",
    "        # Extrair os valores comuns para cada item\n",
    "        first_id = item.get(\"id\")\n",
    "        inventory_id = item.get(\"inventory_id\")\n",
    "        logistic_type = item.get(\"shipping\", {}).get(\"logistic_type\")\n",
    "\n",
    "        # Extrair os valores específicos para cada variação\n",
    "        for variacao in item.get(\"variations\", []):\n",
    "            variation_id = variacao.get(\"id\")\n",
    "            variation_seller_sku = variacao.get(\"seller_custom_field\")\n",
    "            variation_inventory_id = variacao.get(\"inventory_id\")\n",
    "            attribute_combination = variacao.get(\"attribute_combinations\", [{}])[0]\n",
    "            value_id = attribute_combination.get(\"value_id\")\n",
    "            value_name = attribute_combination.get(\"value_name\")\n",
    "            item_relations = attribute_combination.get(\"item_relations\", [{}])[0]\n",
    "\n",
    "            # Adicionar os resultados_variations à lista\n",
    "            resultados_variations.append(\n",
    "                {\n",
    "                    \"ml_code\": first_id,\n",
    "                    \"inventory_id\": inventory_id,\n",
    "                    # \"logistic_type\": logistic_type,\n",
    "                    \"variation_id\": variation_id,\n",
    "                    # \"value_id\": value_id,\n",
    "                    \"value_name\": value_name,\n",
    "                    # \"var_seller_sku\": variation_seller_sku,\n",
    "                    \"variation_inventory_id\": variation_inventory_id,\n",
    "                    # \"item_relations\":item_relations,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df_variations = pd.DataFrame(resultados_variations)\n",
    "\n",
    "    # Unindo as duas tabelas\n",
    "    df_sku_var = pd.merge(\n",
    "        df_sku,\n",
    "        df_variations,\n",
    "        left_on=[\"ml_code\", \"inventory_id\"],\n",
    "        right_on=[\"ml_code\", \"inventory_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    df_sku_var = df_sku_var.drop([\"variations\", \"variation_id\"], axis=1)\n",
    "    df_sku_var\n",
    "\n",
    "    # *se variation_inventory_id = None -> variation_inventory_id == inventory_id && remove inventory_id && variation_inventory_id rename to inventory_id*\n",
    "    df_sku_var[\"variation_inventory_id\"].fillna(\n",
    "        df_sku_var[\"inventory_id\"], inplace=True\n",
    "    )\n",
    "\n",
    "    # Editando tabela\n",
    "    cols = [\n",
    "        \"ml_code\",\n",
    "        \"variation_inventory_id\",\n",
    "        \"value_name\",\n",
    "        \"status\",\n",
    "        \"catalog_listing\",\n",
    "    ]\n",
    "    df_sku_var = df_sku_var[cols]\n",
    "    df_sku_var = df_sku_var.rename(columns={\"variation_inventory_id\": \"inventory_id\"})\n",
    "\n",
    "    logger.info(f\"Tamanho do dataframe final: {df_sku_var.shape}\")\n",
    "\n",
    "\n",
    "    ### Populando banco de dados ###\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "\n",
    "        # Use a tabela fornecida como parâmetro\n",
    "        query = f\"SELECT * FROM {table_item};\"\n",
    "        logger.info(query)\n",
    "        df_items = pd.read_sql(query, conn)\n",
    "    except psycopg2.Error as e:\n",
    "        logger.error(f\"Erro do psycopg2 em 'items': {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao consultar 'items': {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "    dx = df_items.copy()\n",
    "    dy = df_sku_var.copy()\n",
    "\n",
    "    # Editando DFs\n",
    "    dx = dx.drop(columns=[\"created_at\", \"updated_at\"])  # remove linhas de data\n",
    "    dx.replace(\"NaN\", np.nan, inplace=True)  # altera de strin para NaN\n",
    "    dx = dx.astype(str)  # altera para tipo string\n",
    "    dy = dy.astype(str)\n",
    "\n",
    "    # Merge com base nas colunas ml_code e inventory_id\n",
    "    merged_df = pd.merge(\n",
    "        dy,\n",
    "        dx,\n",
    "        on=[\"ml_code\", \"inventory_id\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_sku_var\", \"_items\"),\n",
    "    )\n",
    "\n",
    "    # Linhas com valores diferentes\n",
    "    different_rows = merged_df[\n",
    "        (merged_df[\"value_name_sku_var\"] != merged_df[\"value_name_items\"])\n",
    "        | (merged_df[\"status_sku_var\"] != merged_df[\"status_items\"])\n",
    "        | (merged_df[\"catalog_listing_sku_var\"] != merged_df[\"catalog_listing_items\"])\n",
    "    ]\n",
    "\n",
    "    # Compare os DataFrames\n",
    "    identicos = dx.equals(dy)\n",
    "    # Exiba o resultado\n",
    "    logger.info(\"Os DataFrames são idênticos:\", identicos)\n",
    "\n",
    "    # Encontrar diferenças usando merge\n",
    "    diferencas = (\n",
    "        pd.merge(dx, dy, how=\"outer\", indicator=True)\n",
    "        .query('_merge == \"left_only\"')\n",
    "        .drop(\"_merge\", axis=1)\n",
    "    )\n",
    "\n",
    "    # Criar um novo DataFrame apenas com as colunas modificadas\n",
    "    df_atualizado = dx.copy()\n",
    "    df_atualizado[diferencas.columns] = diferencas\n",
    "\n",
    "    # Remover linhas onde todos os valores em TODAS as colunas são NaN\n",
    "    df_atualizado_sem_nan = df_atualizado.dropna(\n",
    "        how=\"all\", subset=df_atualizado.columns\n",
    "    )\n",
    "\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Iterar sobre as linhas do DataFrame e executar as atualizações no banco de dados\n",
    "    for index, row in df_atualizado_sem_nan.iterrows():\n",
    "        ml_code = row[\"ml_code\"]\n",
    "        inventory_id = row[\"inventory_id\"]\n",
    "        value_name = row[\"value_name\"]\n",
    "        status = row[\"status\"]\n",
    "        catalog_listing = row[\"catalog_listing\"]\n",
    "        updated_at = datetime.now()  # Use a data/hora atual\n",
    "\n",
    "        # Construir a instrução SQL de atualização\n",
    "        query = f\"UPDATE {table_item} SET value_name = %s, status = %s, catalog_listing = %s, updated_at = %s WHERE ml_code = %s AND inventory_id = %s\"\n",
    "        update_query = sql.SQL(\n",
    "            query\n",
    "        )\n",
    "        logger.info(f\"Inserindo dados: {[value for value in row]}\")\n",
    "        # Executar a instrução SQL\n",
    "        cursor.execute(\n",
    "            update_query,\n",
    "            (\n",
    "                value_name,\n",
    "                status,\n",
    "                catalog_listing,\n",
    "                updated_at,\n",
    "                ml_code,\n",
    "                inventory_id,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    logger.info(\"Dados inseridos com sucesso!\")\n",
    "\n",
    "    # Encontrar linhas onde os pares ml_code e inventory_id em df_ficticio são diferentes de dx\n",
    "    diferenca = pd.merge(\n",
    "        dx, dy, on=[\"ml_code\", \"inventory_id\"], how=\"right\", indicator=True\n",
    "    )\n",
    "\n",
    "    # Filtrar apenas as linhas em que df_ficticio tem valores diferentes de dx\n",
    "    diferenca = diferenca.query('_merge == \"right_only\"').drop(columns=\"_merge\")\n",
    "\n",
    "    # Selecionar colunas específicas e renomear\n",
    "    diferenca = diferenca[\n",
    "        [\"ml_code\", \"inventory_id\", \"value_name_y\", \"status_y\", \"catalog_listing_y\"]\n",
    "    ]\n",
    "    diferenca = diferenca.rename(\n",
    "        columns={\n",
    "            \"value_name_y\": \"value_name\",\n",
    "            \"status_y\": \"status\",\n",
    "            \"catalog_listing_y\": \"catalog_listing\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Inserir novos dados no banco de dados\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Use a tabela fornecida como parâmetro\n",
    "    for index, row in diferenca.iterrows():\n",
    "        query = f\"INSERT INTO {table_item} (ml_code, inventory_id, value_name, status, catalog_listing) VALUES (%s, %s, %s, %s, %s)\"\n",
    "        insert_query = sql.SQL(\n",
    "            query\n",
    "        )\n",
    "        logger.info(f\"Inserindo dados: {[value for value in row]}\")\n",
    "        cursor.execute(\n",
    "            insert_query,\n",
    "            (\n",
    "                row[\"ml_code\"],\n",
    "                row[\"inventory_id\"],\n",
    "                row[\"value_name\"],\n",
    "                row[\"status\"],\n",
    "                row[\"catalog_listing\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    # Feche o cursor e a conexão\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    logger.info(\"Dados inseridos com sucesso!\")\n",
    "\n",
    "    end_prog = time.time()  # Registra o tempo depois de toda aplicação\n",
    "    elapsed_time = end_prog - start_prog  # Calcula o tempo decorrido\n",
    "    logger.info(f\"Tempo Total do processo: {elapsed_time / 60} minutos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_data_to_db(ACCESS_TOKEN_BUENOSHOPS, SELLER_ID_BUENOSHOPS, db_config, 'bueno_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_data_to_db(ACCESS_TOKEN_MUSICALCRIS, SELLER_ID_MUSICALCRIS, db_config, 'cris_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_data_to_db(ACCESS_TOKEN_MCENTER, SELLER_ID_MCENTER, db_config, 'mcenter_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
